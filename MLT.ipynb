{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot(X, y, model, feature1, feature2,  title):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def load_dataset(feature1, feature2):\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    X_subset = X[:, [feature1, feature2]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbours Algorithm: \n",
    "\n",
    "Pros: \n",
    "1) The calculation time is less.\n",
    "2) Predictive power is very high, which makes it effective and efficient.\n",
    "3) It is a versatile algorithm as we can use it for classification and regression.\n",
    "4) It has relatively high accuracy.\n",
    "\n",
    "Cons:\n",
    "1) It is expensive in determining K if the dataset is large. It requires more memory storage.\n",
    "2) It is very sensitive to the data’s scale and irrelevant features.\n",
    "3) The computation cost is quite high as each training example’s distance is calculated.\n",
    "4) Typically difficult to handle high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_dataset(0, 2)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy: {knn.score(X_test, y_test):.2f}\")\n",
    "\n",
    "plot(X_train, y_train, knn, 'Sepal Length', 'Petal Length', 'Nearest Neighbour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM):\n",
    "\n",
    "Pros:\n",
    "1) SVM works relatively well when there is a clear margin of separation between classes.\n",
    "2) SVM is more effective in high dimensional spaces.\n",
    "3) SVM is relatively memory efficient.\n",
    "\n",
    "Cons:\n",
    "1) SVM algorithm is not suitable for large data sets.\n",
    "2) In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_dataset(0, 2)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy: {knn.score(X_test, y_test):.2f}\")\n",
    "\n",
    "plot(X_train, y_train, svm, 'Sepal Length', 'Petal Length', 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees Algorithm: \n",
    "\n",
    "Pros: \n",
    "1) Outliers tend to have minimal impact on the overall model performance.\n",
    "2) Handles both numerical and categorical data.\n",
    "3) Decision trees are not sensitive to the scale of features.\n",
    "\n",
    "Cons:\n",
    "1) Decision trees are prone to overfitting.\n",
    "2) Decision trees are sensitive to small variations in the data.\n",
    "3) Features with a large number of levels tend to be favored over features with fewer levels in decision tree splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_dataset(0, 2)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy: {knn.score(X_test, y_test):.2f}\")\n",
    "\n",
    "plot(X_train, y_train, tree, 'Sepal Length', 'Petal Length', 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All showed similar accuracy but...\n",
    "\n",
    "Decision Trees algorithm has the best speed.\n",
    "SVM algorithm has the second best speed.\n",
    "The KNN algorithm has the worst speed.\n",
    "\n",
    "In the end each algorithm is better for different sitiuations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
